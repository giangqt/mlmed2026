{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb1ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392f73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Setup and Configuration\n",
    "# ==========================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "IMG_SIZE = 256\n",
    "SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Dataset Definition\n",
    "# ==========================================\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images[index]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Binarize mask\n",
    "        mask = (mask > 0).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fee18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Model Architecture (U-Net)\n",
    "# ==========================================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.up_conv1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.up_conv2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.up_conv3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.up_conv4 = DoubleConv(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        p1 = self.pool1(d1)\n",
    "        d2 = self.down2(p1)\n",
    "        p2 = self.pool2(d2)\n",
    "        d3 = self.down3(p2)\n",
    "        p3 = self.pool3(d3)\n",
    "        d4 = self.down4(p3)\n",
    "        p4 = self.pool4(d4)\n",
    "\n",
    "        b = self.bottleneck(p4)\n",
    "\n",
    "        u1 = self.up1(b)\n",
    "        u1 = torch.cat((d4, u1), dim=1)\n",
    "        u1 = self.up_conv1(u1)\n",
    "\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat((d3, u2), dim=1)\n",
    "        u2 = self.up_conv2(u2)\n",
    "\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat((d2, u3), dim=1)\n",
    "        u3 = self.up_conv3(u3)\n",
    "\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat((d1, u4), dim=1)\n",
    "        u4 = self.up_conv4(u4)\n",
    "\n",
    "        return self.final_conv(u4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Training Helper Functions\n",
    "# ==========================================\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for images, masks in loop:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_score = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                val_score += dice_coef(masks, preds.float()).item()\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        avg_dice = val_score / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} Completed. Avg Loss: {avg_loss:.4f}, Val Dice Score: {avg_dice:.4f}\")\n",
    "\n",
    "def visualize_results(model, loader, device, num_samples=4):\n",
    "    model.eval()\n",
    "    images, masks = next(iter(loader))\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(num_samples, images.shape[0])):\n",
    "        # Input\n",
    "        plt.subplot(3, num_samples, i+1)\n",
    "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "        plt.title(\"Input X-ray\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Ground Truth\n",
    "        plt.subplot(3, num_samples, i+1+num_samples)\n",
    "        plt.imshow(masks[i].cpu().squeeze(), cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Prediction\n",
    "        plt.subplot(3, num_samples, i+1+2*num_samples)\n",
    "        plt.imshow(preds[i].cpu().squeeze(), cmap='gray')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Main Execution\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(SEED)\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Define Paths\n",
    "    BASE_PATH = \"D:\\\\ml_med_data\\\\Infection Segmentation Data\"\n",
    "    TRAIN_COVID_PATH = os.path.join(BASE_PATH, \"Train/COVID-19\")\n",
    "    IMAGES_DIR = os.path.join(TRAIN_COVID_PATH, \"images\")\n",
    "    MASKS_DIR = os.path.join(TRAIN_COVID_PATH, \"infection masks\")\n",
    "\n",
    "    # Check if paths exist\n",
    "    if not os.path.exists(IMAGES_DIR) or not os.path.exists(MASKS_DIR):\n",
    "        print(\"Dataset paths not found. Please check your directory structure.\")\n",
    "        exit()\n",
    "\n",
    "    # Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Dataset & DataLoader\n",
    "    dataset = CovidDataset(IMAGES_DIR, MASKS_DIR, transform=transform)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset Loaded. Training samples: {train_size}, Validation samples: {val_size}\")\n",
    "\n",
    "    # Model Init\n",
    "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Train\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
    "\n",
    "    # Visualize\n",
    "    print(\"Visualizing results...\")\n",
    "    visualize_results(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{ECG Heartbeat Classification Analysis}

\begin{document}
\maketitle

\section{Introduction}
This report documents the practical implementation of a Convolutional Neural Network (CNN) for classifying ECG heartbeats using the PTBDB dataset. The workflow utilizes Python with TensorFlow and Pandas to perform data loading, preprocessing, model architecture design, and evaluation.

\section{Data Preparation}

\subsection{Preprocessing and Splitting}
The analysis processes abnormal and normal heartbeat samples. The data is concatenated and reshaped to meet the 3D input requirements of the 1D CNN: $(Samples, 187, 1)$.

The data is split into training and testing sets (80/20 ratio) using \texttt{train\_test\_split}. The following code snippet demonstrates the splitting and reshaping logic:

\begin{lstlisting}[language=Python, caption=Data splitting and reshaping]
# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Reshape for CNN: (samples, time_steps, features)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
\end{lstlisting}

\section{Model Architecture}

A Sequential CNN model is constructed using TensorFlow Keras. The architecture includes three convolutional blocks followed by dense layers. 

\begin{itemize}
    \item \textbf{Input}: Shape (187, 1)
    \item \textbf{Conv Blocks}: \texttt{Conv1D} (32/64 filters) $\rightarrow$ \texttt{BatchNormalization} $\rightarrow$ \texttt{MaxPooling1D}.
    \item \textbf{Classifier}: Flatten $\rightarrow$ Dense (64, ReLU) $\rightarrow$ Dropout (0.5) $\rightarrow$ Dense (1, Sigmoid).
\end{itemize}

\begin{lstlisting}[language=Python, caption=CNN Model Architecture]
model = models.Sequential([
    layers.Input(shape=(X_train.shape[1], 1)),
    layers.Conv1D(32, 5, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(2),
    layers.Conv1D(64, 5, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])
\end{lstlisting}

\section{Training Configuration}

The model is compiled with the \texttt{Adam} optimizer and \texttt{binary\_crossentropy} loss function. Training is executed for 5 epochs with a batch size of 32.

\begin{lstlisting}[language=Python, caption=Model Training]
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=0)
\end{lstlisting}

\section{Results and Visualization}

To evaluate model stability, the accuracy and loss metrics were extracted from the training history and visualized using Matplotlib.

\subsection{Plotting Code}
The code below generates the comparison plots between training and validation performance:

\begin{lstlisting}[language=Python, caption=Visualization Logic]
plt.figure(figsize=(12, 4))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')

plt.show()
\end{lstlisting}

\subsection{Performance Output}
The resulting visualization indicates the model's convergence over the 5 epochs. Figure \ref{fig:output} displays the generated graphs.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{training.pdf}
    \caption{Training and validation accuracy and loss curves.}
    \label{fig:output}
\end{figure*}

\section{Evaluation}

The model was finally evaluated on the unseen test dataset ($X\_test$, $y\_test$), achieving an accuracy of approximately 92.44\%.

\begin{lstlisting}[language=Python, caption=Final Evaluation]
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {accuracy:.4f}")
\end{lstlisting}

\end{document}